# 4. Consumers and Groups

Kafka consumers read records reliably and at scale by coordinating group membership, tracking offsets, and tuning fetch behavior for throughput and latency. In Spring for Kafka, listener containers manage the poll loop, heartbeats, acks, error handling, and rebalances so that business logic stays concise and testable.

## 4.1 Poll Loop, Heartbeats, Liveness

### 4.1.1 Listener container basics

- Spring’s listener container owns the poll loop and heartbeats, mapping assigned partitions to consumer threads and invoking <code>@KafkaListener</code> handlers with deserialized records. Keep handlers fast and idempotent; offload heavy work to worker pools so the poll thread stays responsive.
- Liveness is maintained by heartbeats to the coordinator while processing; long pauses (GC, blocking IO) can miss heartbeats and trigger rebalances, so prefer short, non-blocking critical sections in listeners.
- Use container concurrency to parallelize across partitions; the thread-per-partition model prevents duplicate processing within a group while allowing scale-out across instances.

```java
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

@Component
public class OrdersListener {
  @KafkaListener(topics = "orders", groupId = "orders-app", concurrency = "3")
  public void handle(String value) {
    // do fast work here; defer heavy tasks to async pools
  }
}
```

### 4.1.2 Liveness-related configs

- <code>max.poll.interval.ms</code> is the max time between polls; if exceeded, the consumer is considered failed and its partitions are revoked, so ensure processing completes within this window or split work.
- <code>session.timeout.ms</code> is how long the coordinator waits before declaring a consumer dead; <code>heartbeat.interval.ms</code> sets the heartbeat cadence and must be lower than the session timeout.
- Tight values detect failures quickly but are sensitive to pauses; looser values tolerate jitter but delay failover, so choose based on SLA and JVM tuning.

```yaml
spring:
  kafka:
    consumer:
      properties:
        max.poll.interval.ms: 300000
        session.timeout.ms: 15000
        heartbeat.interval.ms: 5000
```

## 4.2 Offset Management: Auto vs Manual Commit

### 4.2.1 Auto commit (simple, best-effort)

- With <code>enable.auto.commit=true</code>, offsets are committed periodically regardless of processing success, which is simple and low-ceremony but risks skipping messages if the app crashes mid-batch.
- It suits non-critical telemetry where occasional gaps are acceptable; for stricter guarantees, prefer manual commits after successful processing to achieve at-least-once.
- Keep poll intervals short and limit handler time to ensure commits represent actual progress rather than large speculative jumps.

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: true
      auto-offset-reset: earliest
```

```java
@KafkaListener(topics = "orders", groupId = "orders-auto")
public void handleAuto(String value) {
  // processing; commits are periodic and not tied to completion
}
```

### 4.2.2 Manual commit (at-least-once)

- With <code>enable.auto.commit=false</code>, acknowledge offsets only after processing completes, ensuring replays after crashes rather than data loss; handlers must be idempotent to tolerate duplicates.
- Use <code>AckMode.MANUAL_IMMEDIATE</code> to commit per-record for minimal replay, or <code>AckMode.BATCH</code> to commit per-batch for higher throughput; align with downstream idempotency guarantees.
- Pair with cooperative rebalancing to reduce disruption during scaling or deployments while maintaining correctness.

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: false
```

```java
import org.springframework.kafka.support.Acknowledgment;

@KafkaListener(topics = "orders", groupId = "orders-manual", containerFactory = "manualAckContainerFactory")
public void handleManual(String value, Acknowledgment ack) {
  // process safely (idempotent)
  ack.acknowledge(); // commit after success
}
```

```java
import org.springframework.context.annotation.Bean;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;

@Bean
public ConcurrentKafkaListenerContainerFactory<String, String> manualAckContainerFactory(
    ConsumerFactory<String, String> cf) {
  var factory = new ConcurrentKafkaListenerContainerFactory<String, String>();
  factory.setConsumerFactory(cf);
  factory.getContainerProperties().setAckMode(
      org.springframework.kafka.listener.ContainerProperties.AckMode.MANUAL_IMMEDIATE);
  return factory;
}
```

## 4.3 Seek, Replay, and Starting Position

### 4.3.1 Seeking on startup or on demand

- Seeking repositions the consumer to reprocess data for backfills, audits, or bug fixes; seek to beginning/end, an exact offset, or the first after a timestamp.
- Implement <code>ConsumerSeekAware</code> to seek when partitions are assigned, or use <code>@TopicPartition</code> with <code>initialOffset</code> for fixed offsets; coordinate seeks with commits to avoid conflicting state.
- Always announce and monitor replays, as they can increase lag and impact downstream consumers if producing new results.

```java
import org.springframework.kafka.listener.ConsumerSeekAware;
import org.apache.kafka.common.TopicPartition;
import org.springframework.stereotype.Component;
import java.util.Map;

@Component
public class ReplayOnAssignListener implements ConsumerSeekAware {
  @Override
  public void onPartitionsAssigned(Map<TopicPartition, Long> assignments, ConsumerSeekCallback cb) {
    assignments.keySet().forEach(tp -> cb.seekToBeginning(tp.topic(), tp.partition()));
  }
}
```

## 4.4 Consumer Groups Deep Dive

### 4.4.1 What is a consumer group?

- A consumer group is a set of instances sharing a <code>group.id</code> that coordinate to ensure each partition is processed by at most one member at a time, providing parallelism and fault tolerance.
- Each group maintains its own committed offsets in <code>\_\_consumer_offsets</code>, so multiple groups can consume the same topic independently without interfering with each other.
- The max useful members per group equals the number of partitions; extra members will be idle unless the topic is partitioned further.
- Stable <code>group.id</code> naming is essential: changing it creates a brand new consumption lineage with fresh offsets based on the reset policy.

```yaml
spring:
  kafka:
    consumer:
      group-id: orders-app
```

### 4.4.2 Partition-to-member mapping

- The group coordinator assigns partitions to members when the group stabilizes; only one member processes a given partition to avoid duplicates within the group.
- Sticky assignors minimize partition movement across rebalances, preserving cache locality and reducing warm-up penalties for stateful handlers.
- Plan partition counts to match target concurrency across instances and threads; re-evaluate as traffic grows or shrinks to avoid hotspots and idling.
- Use keys wisely to distribute load evenly; shard hot keys (e.g., <code>userId#0..N</code>) if a few entities dominate traffic.

### 4.4.3 Rebalancing: eager vs cooperative

- Eager rebalancing revokes all partitions from all members before reassignment, causing a brief stop-the-world pause; it is simple but disruptive under frequent membership changes.
- Cooperative (incremental) rebalancing only moves necessary partitions, allowing consumers to keep processing unaffected partitions and reducing tail latency during deploys or failures.
- Prefer the cooperative sticky assignor in most cases; monitor rebalance frequency/duration and correlate with deploy cadence, GC pauses, and autoscaling events to tune stability.
- Always handle revocation and assignment callbacks quickly to avoid prolonging rebalances and increasing lag.

```yaml
spring:
  kafka:
    consumer:
      properties:
        partition.assignment.strategy: org.apache.kafka.clients.consumer.CooperativeStickyAssignor
```

## 4.5 Offsets: Storage, Ownership, and Semantics

### 4.5.1 Where offsets live and how to manage them

- Committed offsets live in the internal <code>\_\_consumer_offsets</code> topic keyed by <code>group.id</code> and partition, enabling fast recovery and portability across instances.
- With manual acks, commit only after successful processing to achieve at-least-once semantics; design handlers to be idempotent because replays can occur after crashes.
- Changing <code>group.id</code> creates a new lineage; use <code>auto-offset-reset</code> to control starting position when no offsets exist (earliest vs latest).

```yaml
spring:
  kafka:
    consumer:
      enable-auto-commit: false
      auto-offset-reset: latest
```

### 4.5.2 Reset policies and replay

- <code>earliest</code> is useful for new groups or backfills to read the full retained history; <code>latest</code> is typical in production to consume new data only.
- Programmatic seeking to offsets or timestamps enables targeted reprocessing; coordinate with downstream idempotency to avoid duplicated effects.
- Announce replays to stakeholders and watch lag metrics closely to ensure clusters and consumers can handle the increased load.

## 4.6 Throughput Tuning: Concurrency and Fetch

### 4.6.1 Concurrency scaling patterns

- Spring’s <code>concurrency</code> creates multiple consumer threads in one instance; effective only up to the number of assigned partitions.
- Horizontal scaling adds more instances with the same <code>group.id</code>, spreading partitions across processes for fault isolation and better CPU/memory utilization.
- Combine both: scale instances first for resilience, then adjust concurrency per instance to utilize cores without oversubscribing threads.

```java
@KafkaListener(topics = "orders", groupId = "orders-app", concurrency = "4")
public void handle(String value) { /* ... */ }
```

### 4.6.2 Fetch and flow control

- <code>max.poll.records</code> caps the number of records returned per poll; tune to balance throughput (higher) against memory/GC pressure (lower).
- <code>fetch.min.bytes</code> and <code>fetch.max.wait.ms</code> let the broker accumulate data for more efficient fetches at the cost of latency; useful for batch-oriented consumers.
- <code>max.partition.fetch.bytes</code> prevents large messages from blowing up memory by bounding per-partition fetch size; align with max message sizes.

```yaml
spring:
  kafka:
    consumer:
      max-poll-records: 500
      properties:
        fetch.min.bytes: 1048576
        fetch.max.wait.ms: 50
        max.partition.fetch.bytes: 1048576
```

## 4.7 Error Handling, Retries, and DLT

### 4.7.1 Blocking retries with DefaultErrorHandler

- Blocking retries keep processing in-place with backoff and a retry cap before delegating to a recovery handler (often a DLT publisher), preserving partition order but risking stalls on poison messages.
- Use small retry counts for hot partitions and escalate quickly to DLT; include rich headers (error type, original offset) to speed up triage and replay tooling.
- Pair with idempotent handlers to handle potential replays after crashes or restarts.

```java
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.util.backoff.FixedBackOff;

@Bean
DefaultErrorHandler errorHandler(org.springframework.kafka.core.KafkaTemplate<Object, Object> template) {
  var recoverer = new DeadLetterPublishingRecoverer(template); // <topic>.DLT default
  return new DefaultErrorHandler(recoverer, new FixedBackOff(1000L, 3L));
}
```

### 4.7.2 Non-blocking retries with @RetryableTopic

- Non-blocking retries route failures to retry topics with delayed backoff so the main consumer keeps moving; after attempts are exhausted, the message lands in a DLT.
- This preserves throughput and reduces head-of-line blocking, ideal for high-SLA streams where one poison record must not stall the partition.
- Ensure retry and DLT topics have appropriate retention and monitoring so operators can replay or analyze failures effectively.

```java
import org.springframework.kafka.annotation.RetryableTopic;
import org.springframework.retry.annotation.Backoff;
import org.springframework.kafka.retrytopic.TopicSuffixingStrategy;

@RetryableTopic(
    attempts = "4",
    backoff = @Backoff(delay = 1000, multiplier = 2.0),
    topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE)
@KafkaListener(topics = "orders", groupId = "orders-rtr")
public void handleRetry(String value) {
  // throw to trigger retry topics and eventual DLT
}
```

## 4.8 Pause/Resume and Backpressure

### 4.8.1 Pausing partitions safely

- Use pause/resume when downstream services are saturated to temporarily stop fetching while staying in the group, preventing unchecked lag growth.
- Resume as soon as downstream pressure relents to avoid breaching <code>max.poll.interval.ms</code> and triggering a rebalance that may worsen recovery.
- Instrument pause durations and reasons to inform capacity planning and identify chronic bottlenecks.

```java
import org.apache.kafka.clients.consumer.Consumer;

@KafkaListener(topics = "orders", groupId = "orders-pause")
public void handleWithPause(String value, Consumer<?, ?> consumer) {
  consumer.pause(consumer.assignment());
  // drain downstream backlog...
  consumer.resume(consumer.assignment());
}
```

## 4.9 Reading Only Committed Data

### 4.9.1 Isolation level

- <code>isolation.level=read_committed</code> hides uncommitted or aborted transactional writes, ensuring consumers only see results that are part of committed transactions.
- Use this in exactly-once pipelines or when partial results would cause operational or financial inconsistencies; otherwise, <code>read_uncommitted</code> offers slightly lower latency.
- Ensure transactional producers and consumers are co-tuned; long transactions can delay visibility and increase perceived lag.

```yaml
spring:
  kafka:
    consumer:
      properties:
        isolation.level: read_committed
```

## 4.10 Practical Checklist

- Use manual acks for at-least-once and idempotent handlers to tolerate replays; auto-commit is fine for best-effort telemetry.
- Prefer cooperative rebalancing to reduce disruption; observe and tune rebalance frequency/duration alongside deploy and autoscaling policies.
- Scale with instance count first for resilience, then with concurrency per instance; cap <code>max.poll.records</code> and tune fetch sizes for stable memory.
- Choose blocking retries for strict ordering pipelines and non-blocking retry topics for high-SLA streams; always publish rich metadata to DLT.
- Enable pause/resume as a safety valve and pair with strong observability to prevent silent stalls and missed SLAs.
